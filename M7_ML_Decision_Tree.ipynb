{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee93c0db",
   "metadata": {},
   "source": [
    "\n",
    "# Decision Tree | Assignment Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0cd37",
   "metadata": {},
   "source": [
    "# Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3200dbac",
   "metadata": {},
   "source": [
    "\n",
    "### Question 1\n",
    "**What is a Decision Tree, and how does it work in the context of classification?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed384f",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**  \n",
    "A Decision Tree is a flowchart-like model that splits the feature space into regions using a sequence of if–else rules. In classification, each internal node tests a feature; branches correspond to test outcomes; and leaves assign a class. The tree is learned by greedily choosing splits that best reduce class impurity (e.g., Gini or entropy) on the training data, and predictions for a new sample follow the path dictated by its feature values until a leaf is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f424e",
   "metadata": {},
   "source": [
    "\n",
    "### Question 2\n",
    "**Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eaa711",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**  \n",
    "- **Gini impurity:** \\( G = \\sum_k p_k (1 - p_k) = 1 - \\sum_k p_k^2 \\), where \\(p_k\\) is the fraction of class *k* in a node. Lower Gini means purer nodes.  \n",
    "- **Entropy:** \\( H = -\\sum_k p_k \\log_2 p_k \\). Lower entropy means more certainty.  \n",
    "\n",
    "At each node, the algorithm evaluates candidate splits and chooses the one that **maximizes impurity reduction** (Information Gain for entropy or Gini decrease). This yields children nodes that are purer than the parent, improving class separability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33999fa6",
   "metadata": {},
   "source": [
    "\n",
    "### Question 3\n",
    "**What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1d709",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**  \n",
    "- **Pre-pruning (early stopping):** Limit the tree during growth using constraints like `max_depth`, `min_samples_split`, or `min_impurity_decrease`.  \n",
    "  - *Advantage:* Faster training and lower variance out-of-the-box (prevents very deep, overfit trees).  \n",
    "- **Post-pruning (cost-complexity pruning):** First grow a large tree, then prune back using a penalty on complexity (e.g., `ccp_alpha`) based on validation/CV.  \n",
    "  - *Advantage:* Often yields a better bias–variance trade-off because the algorithm can explore rich splits before pruning subtrees that don’t generalize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6df59",
   "metadata": {},
   "source": [
    "\n",
    "### Question 4\n",
    "**What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8b016",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**  \n",
    "**Information Gain (IG)** is the reduction in impurity from a parent node to its children after a split. Using entropy,  \n",
    "\\[ IG = H(\\text{parent}) - \\sum_i \\frac{N_i}{N} H(\\text{child}_i) \\]  \n",
    "Higher IG indicates a split that creates purer child nodes. Trees pick the split with **maximum IG** (or maximum Gini decrease), which leads to more informative partitions and better predictive performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c12c0d",
   "metadata": {},
   "source": [
    "\n",
    "### Question 5\n",
    "**What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d08ef",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**  \n",
    "- **Applications:** credit risk scoring, churn prediction, medical diagnosis triage, fraud detection, lead qualification, and simple rule-based recommenders.  \n",
    "- **Advantages:** easy to interpret/visualize; handles mixed data types; little feature scaling needed; captures non-linear relations and interactions.  \n",
    "- **Limitations:** prone to overfitting without pruning; decision boundaries are axis-aligned; small data changes can alter the tree (instability); typically lower accuracy than ensembles like Random Forests/Gradient Boosting on complex tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ed746",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f3c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def describe_feature_importances(model, feature_names):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        return pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "    else:\n",
    "        return pd.Series(dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01e4e4",
   "metadata": {},
   "source": [
    "\n",
    "### Q6\n",
    "**Write a Python program to:**\n",
    "- Load the Iris Dataset  \n",
    "- Train a Decision Tree Classifier using the Gini criterion  \n",
    "- Print the model’s accuracy and feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f5ce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Gini): 0.8947\n",
      "\n",
      "Feature Importances (descending):\n",
      " petal length (cm)    0.919887\n",
      "petal width (cm)     0.046629\n",
      "sepal width (cm)     0.020091\n",
      "sepal length (cm)    0.013394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load data\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Model with Gini\n",
    "clf_gini = DecisionTreeClassifier(criterion=\"gini\", random_state=RANDOM_STATE)\n",
    "clf_gini.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf_gini.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy (Gini): {acc:.4f}\")\n",
    "fi = describe_feature_importances(clf_gini, feature_names)\n",
    "print(\"\\nFeature Importances (descending):\\n\", fi.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc42e77",
   "metadata": {},
   "source": [
    "\n",
    "### Q7\n",
    "**Write a Python program to:**\n",
    "- Load the Iris Dataset  \n",
    "- Train a Decision Tree Classifier with `max_depth=3` and compare its accuracy to a fully-grown tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177fa4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (max_depth=3): 0.8947\n",
      "Accuracy (fully-grown): 0.8947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load data\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Shallow tree\n",
    "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\n",
    "clf_depth3.fit(X_train, y_train)\n",
    "acc_depth3 = accuracy_score(y_test, clf_depth3.predict(X_test))\n",
    "\n",
    "# Fully-grown tree (no max_depth)\n",
    "clf_full = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "clf_full.fit(X_train, y_train)\n",
    "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy (max_depth=3): {acc_depth3:.4f}\")\n",
    "print(f\"Accuracy (fully-grown): {acc_full:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761e87a",
   "metadata": {},
   "source": [
    "\n",
    "### Q8\n",
    "**Write a Python program to:**\n",
    "- Load the California Housing dataset from `sklearn`  \n",
    "- Train a Decision Tree Regressor  \n",
    "- Print the Mean Squared Error (MSE) and feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "709170b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.5285\n",
      "\n",
      "Feature Importances (descending):\n",
      " MedInc        0.526241\n",
      "AveOccup      0.134914\n",
      "Latitude      0.088012\n",
      "Longitude     0.086799\n",
      "HouseAge      0.050926\n",
      "AveRooms      0.048155\n",
      "Population    0.036914\n",
      "AveBedrms     0.028039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Load data\n",
    "cal = fetch_california_housing(as_frame=True)\n",
    "X = cal.data\n",
    "y = cal.target  # Median House Value (in 100k USD)\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "# Model\n",
    "regr = DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "pred = regr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "\n",
    "fi = describe_feature_importances(regr, feature_names)\n",
    "print(\"\\nFeature Importances (descending):\\n\", fi.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395d972",
   "metadata": {},
   "source": [
    "\n",
    "### Q9\n",
    "**Write a Python program to:**\n",
    "- Load the Iris Dataset  \n",
    "- Tune the Decision Tree’s `max_depth` and `min_samples_split` using `GridSearchCV`  \n",
    "- Print the best parameters and the resulting model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1510d2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Test Accuracy of Best Model: 0.8947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load data\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Grid search\n",
    "param_grid = {\n",
    "    \"max_depth\": [2, 3, 4, 5, None],\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "test_acc = accuracy_score(y_test, best_model.predict(X_test))\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(f\"Test Accuracy of Best Model: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f3cb4",
   "metadata": {},
   "source": [
    "\n",
    "### Question 10\n",
    "**Healthcare use case:** You have a large dataset with mixed data types and some missing values. Explain the step-by-step process to:\n",
    "- Handle missing values\n",
    "- Encode categorical features\n",
    "- Train a Decision Tree model\n",
    "- Tune its hyperparameters\n",
    "- Evaluate its performance  \n",
    "And describe the business value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa2eae",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**  \n",
    "1. **Data audit & split:** Profile missingness by column and class; create a stratified train/validation/test split before heavy preprocessing to avoid leakage.  \n",
    "2. **Missing values:**  \n",
    "   - Numerical: impute with median (robust to outliers).  \n",
    "   - Categorical: impute with a dedicated category like `\"Missing\"` (preserves signal of missingness).  \n",
    "   - Optionally add *missing-indicator* features for high-impact columns.  \n",
    "3. **Encoding:** Use **ordinal/one-hot encoding** for categorical variables. Decision Trees don’t require scaling; one-hot is safe and preserves order-agnostic semantics.  \n",
    "4. **Model training:** Train a `DecisionTreeClassifier` with a fixed `random_state`. Start simple (gini or entropy), use class-weighting if classes are imbalanced.  \n",
    "5. **Hyperparameter tuning:** Use cross-validation to search over `max_depth`, `min_samples_split`, `min_samples_leaf`, and `ccp_alpha` (cost-complexity pruning). Optimize a metric aligned to the business goal (e.g., recall or balanced accuracy if missing a disease is costly).  \n",
    "6. **Evaluation:** Report accuracy plus **precision/recall/F1**, ROC–AUC, confusion matrix, and calibration (Brier score, reliability plot). Perform threshold analysis to balance false negatives vs. false positives.  \n",
    "7. **Interpretability & monitoring:** Inspect feature importances, path explanations, and partial dependency/ICE for key drivers; set up drift and performance monitoring post-deployment.  \n",
    "\n",
    "**Business value:** Earlier and more accurate triage reduces missed diagnoses (lower false negatives), optimizes resource allocation (flag high-risk patients for further testing), and supports clinicians with transparent, explainable rules—improving patient outcomes and operational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fc5d3",
   "metadata": {},
   "source": [
    "# ------------------------------- Thank You -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f9a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
